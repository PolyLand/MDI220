---
title: "Mini Project"
author: "Yixiao FEI"
date: "Oct 19, 2018"
output: html_document
---

## Initialization
```{r}
set.seed(1, kind="Marsaglia-Multicarry")
setwd("H:/Documents/WorkSpace/MDI220/Projet")
```

## Exercice 1
### 1.1
```{r}
data<-read.csv('NuclearPowerAccidents2016.csv',header=TRUE)
cost_vector <- c()
for (i in 1:nrow(data)) {
  if(as.Date(data[i, 1], "%m/%d/%Y") < "1979-03-28"){
    if(!is.na(data[i, 3])){
      cost_vector <- c(cost_vector, data[i, 3])
    }
  }
}
sprintf("The length of the cost vector is %d.", length(cost_vector))
```
### 1.2 (a)
Supposon que $X \sim \mathcal{N}(\mu, \sigma^2)$

Il existe $Z \sim \mathcal{N}(0, 1)$ tel que $X=\sigma Z + \mu$, $Z$ suit une loi normale centrée réduite.

Or $F^{-1} (p;\mu,\sigma^2)=\inf_{x} [F(x;\mu,\sigma^2) \geq p]=\inf_{x}[P_{\mu, \sigma^2}(X \leq x) \geq p]$

Donc $F^{-1} (p;\mu,\sigma^2)=\inf_{x}[P_{0, 1}(Z \leq \frac{x-\mu}{\sigma}) \geq p]$

Supposons $x^* \in \mathbb{R}$ tel que $x^*=\inf_x[P_{0, 1}(Z \leq \frac{x-\mu}{\sigma}) \geq p]$

Alors, $F^{-1}(p;0,1) = \frac{x^*-\mu}{\sigma}$

Donc $x^*=\mu+\sigma F^{-1}(p;0,1)$

Finalement, $F^{-1} (p;\mu,\sigma^2)=\mu+\sigma F^{-1} (p;0,1)$

### 1.2 (b)
```{r}
t<-(rank(cost_vector)-0.5)/length(cost_vector) # To avoid infinity in q
q<-qnorm(t)

index1<-as.integer(0.25*length(cost_vector))
index2<-as.integer(0.75*length(cost_vector))
b<-(cost_vector[index2]-cost_vector[index1])/(q[index2]-q[index1])
a<-cost_vector[index1]-b*q[index1]

plot(q,cost_vector, type="p", col="red")
abline(a, b, col="blue")

```

### 1.2 (c)
```{r}
qqnorm(cost_vector, col="red", type="p")
qqline(cost_vector, probs = c(0.25,0.75),col="blue",lwd=2)

```

### 1.2 (d)
Le QQ-plot est très différent qu'une droite afine d'une fonction $f(x)=x$, donc le modèle des lois normales ne sont pas adaptées.

### 1.3 (a)
Nous avons $F^{-1}(p;\lambda)=\inf_x[P_{\lambda}(X \leq x) \geq p]$

Or pour $x\geq 0, P_{\lambda}(X \leq x) = \int_0^x \lambda \mathrm{e}^{-\lambda u} \mathrm{d}u$

Sinon, $P_{\lambda}(X \leq x) = 0$

Après le changement de variable, nous avons $P_{\lambda}(X \leq x)=\int_0^{\lambda x}  \mathrm{e}^{- u} \mathrm{d}u=P_{1}(X \leq \lambda x)=F(\lambda x;1)$

Supposons $x^* \geq 0$, tel que $x^*=\inf_x [F(\lambda x;1)\geq p]$

Alors $F^{-1}(p;1)=\lambda x^*$

Donc $x^*=\frac{1}{\lambda}F^{-1}(p;1)$

Finalement, $F^{-1}(p;\lambda)=\frac{1}{\lambda}F^{-1}(p;1)$

### 1.3 (b)
```{r}
t<-(rank(cost_vector)-0.5)/length(cost_vector) # To avoid infinity in q
q<-qexp(t)

index1<-as.integer(0.25*length(cost_vector))
index2<-as.integer(0.75*length(cost_vector))
b<-(cost_vector[index2]-cost_vector[index1])/(q[index2]-q[index1])
a<-cost_vector[index1]-b*q[index1]

plot(q,cost_vector, type="p", col="red")
abline(a, b, col="blue")

```

### 1.3 (c)
```{r}
#qqexp(x, col="red", type="p")
#qqline(x, probs = c(0.25,0.75),col="blue",lwd=2)
plot(q, rep(0, length(q)), type="p", col="red")

```

### 1.3 (d)
```{r}
hist(cost_vector,freq = F,col="blue")
temp<-1/mean(cost_vector)
curve(dexp(x, temp), col="red",add=T)
```

### 1.3 (e)
Une loi exponentielle semble être plus plausible qu'une loi normale. La droite affine dans le QQ-diagramme réalise mieux la fonction $f(x)=x$.

## Exercice 2
### 2.1
$T_1(X)$ est une moyenne empirique dont l'espérance $E[T_1(X)]=E(X)=\frac{1}{\lambda}$.

C'est un estimateur sans biais.

$Var[T_1(X)]=\frac{1}{n^2}*n \frac{1}{\lambda ^2}=\frac{1}{n\lambda^2}$

La variance converge en 0 quand n est grand, uniformément de variance minimale.

Finalement, $T_1(X)$ est un estimateur UVMB.

### 2.2
Par définition, nous avons $R(\lambda,T_1)=E[(T_1-\frac{1}{\lambda})^2]=E(T_1^2)-\frac{2}{\lambda}E(T_1)+\frac{1}{\lambda^2}$

$R(\lambda,\tilde{T}_{1,\alpha})= E[(\alpha T_1-\frac{1}{\lambda})^2]=\alpha^2 E(T_1^2)-\frac{2\alpha}{\lambda}E(T_1)+\frac{1}{\lambda^2}$

$R(\lambda,\tilde{T}_{1,\alpha})-R(\lambda,T_1)=(\alpha^2-1)E(T_1^2)-(\alpha-1)\frac{2}{\lambda}E(T_1)=$

Or $E(T_1)=\frac{1}{\lambda}$, $E(T_1^2)=\frac{1}{\lambda^2}(1+\frac{1}{n})$

Alors $R(\lambda,\tilde{T}_{1,\alpha})-R(\lambda,T_1)=\frac{1}{\lambda^2}(\alpha-1)((\alpha+1)(\frac{1}{n}+1)-2)$

Donc pour tout $\alpha \in [\frac{n-1}{n+1}, 1]$, $\forall \lambda > 0, \, R(\lambda,\tilde{T}_{1,\alpha})-R(\lambda,T_1)<0$

Le résultat n'est pas en contradiction avec la question précédente parce que $\tilde{T}_{1,\alpha}$ est un estimateur avec biais.

### 2.3
$E(\varphi(X))=m_1=E(X)=\frac{1}{\lambda}$

Comme $\hat{m}_1=\frac{X_1+...+X_n}{n}$

Alors $T_2(X)=\log2 \hat{m}_1=\log2 T_1(X)$

Nous avons $n=55$, $\frac{n-1}{n+1}\approx 0.96$ et $\log2 \approx 0.3$

Donc le risque quadratique de $T_2$ comme un estimateur est plus grand que celui de $T_1$

### 2.4
```{r}
g1<-mean(cost_vector)
sprintf("g1 = %f", g1)
```

### 2.5
```{r}
g2<-log10(2)*mean(cost_vector)
sprintf("g2 = %f", g2)
```

La médiane est plus petite.

### 2.6
```{r}
library(moments)
R1<-c()
R2<-c()
for (N in 1:55){
  mean_cost<-mean(cost_vector[seq(N)])
  moment_cost<-moment(cost_vector[seq(N)],order=2)
  R1<-c(R1, moment_cost-2*g1*mean_cost+g1^2)
  R2<-c(R2, log10(2)^2*moment_cost-log10(2)*2*g1*mean_cost+g1^2)
}
N<-seq(55)
plot( N, R1, type="l", col="red", ylab = "")
par(new=TRUE)
plot( N, R2, type="l", col="green" , ylab = "")
#text(x=40, y=max(R1), labels = "R1", col="red")
#text(x=40, y=max(R1)-2, labels = "R2", col="green")
```

Nous constate bien que les deux risques convergent vers 0 quand $N$ tend vers 0.

Et quand N est petit, le risque $R_2$ est plus petit que $R_1$ jusqu'à $N$ passe environ 30.

## Exercice 3
### 3.1
La démonstration peut être fait par la recherche de la fonction caractéristique.

$\phi_{T(X)} (t)=E[\exp(itT(X))]=\displaystyle\prod_{i=1}^{n}\phi_{X_i}(t) =\frac{1}{(1-\frac{it}{\lambda})^n}$

Donc $T(X)$ est une loi gamma de paramètre $(n,\frac{1}{\lambda})$

Soit $Q_{T(X)}$ la fonction quantile du test.

L'intervalle de dispersion de niveau $\alpha$ tout intervalle de la forme:

$\displaystyle [\,Q_T(\alpha')\,,\,Q_T(\alpha+\alpha')\,]\;,$   avec $\displaystyle 0\leq \alpha'\leq 1-\alpha\;$

Un intervalle de dispersion de niveau  $\alpha$ pour $T$ est tel que $T$ appartient à cet intervalle avec probabilité  $\alpha$. Il contient donc une forte proportion des valeurs que prendra $T$, même s'il est en général beaucoup plus petit que le support de la loi.

### 3.2
```{r}
T<-sum(cost_vector)
l<-qgamma(p=c(0.05), shape = 55, scale=1/0.001)
print(T)
print(l)
```























